
= Deformation Model Functional Model straw man
:author: Chris Crook
:email: ccrook@linz.govt.nz
:imagesdir: images
:toc:



== Overview	

This is a proposal for a functional model to represent deformation models used in the context of coordinate operations - either time dependent transformations or point motion models.

The definition of "deformation model" adopted here is:
____
A deformation model is a model of the displacement of the earth’s surface within a defined spatial and temporal extent.  It predicts the location of a point fixed to the surface at any time within the temporal extent in terms of an accessible coordinate system.  The position of the point is represented as a displacement from a reference position for that point.
____ 

This is a straw man proposal for consideration and refinement by the OGC CRS DWG Deformation Model team. 

Deformation models are typically developed by national geodetic agencies to be used by geospatial and positioning communities to transform between national reference frames and global reference frames.  Currently there is no standard for how such models are described or encoded.  Historically agencies have developed bespoke formats and software to apply coordinate operations.  This work aims to provide a common format which will simplify publishing models to a wider audience.

The potential audience for this proposal are geodetic agencies publishing deformation models, and developers of geospatial and positioning software that consumes them.  

Currently there is no common format for such models, and typically agencies develop both the model and software to evaluate it at any given time and location.  The software may not interface well with consumer software where the transformation may be required, such as in GIS (Geographic Information Systems) and positioning software.  The formats are built to serve the needs of the individual agency, generally using custom binary formats with limited metadata. 

This proposal describes a functional model for describing a deformation model.  It does not propose any particular encoding of the model, though it is hoped that it will lead to the development of an encoding standard. 

== Scope

_This section is here mainly to provide for exclusions.  The wording of this section is not 
intended to be final._

The scope of the deformation model functional model embraces deformation models used to define time dependent coordinate transformations or point motion models that describe the trajectories of points on a deforming surface or body relative to an external reference frame.

=== Exclusions

This scope of this functional model definition does not include

* Rigid body transformation between reference frames such as time dependent helmert or Bursa-Wolf transformations
* Trajectory models for individual points, such as ITRF reference station coordinate models
* Static distortion models used to represent the deformation between two datums (typically between older datums based on local terrestrial observations and modern datums based on global satellite systems)

== Functional definition of the deformation model

The functional model proposed for a general deformation model can be characterised as follows.  

0. [[funcmod-extents]] The deformation model is defined within a specified spatial and temporal extent.  Beyond these extents the model is undefined and cannot be evaluated.

5. [[funcmod-trajectory]]The deformation model defines a trajectory for each point on the physical surface by adding the calculated displacement as function of time to the position used to evaluate the spatial model.  This trajectory is terms of an explicitly defined accessible coordinate system. 

6. [[funcmod-ref-crs]]The position used to calculate the spatial model is not defined in an currently accessible coordinate system - it may be in terms on an accessible coordinate system at a specific epoch.  Its value is only accessible by an inverse calculation using the deformation model.  See the <<formula-inverse, inverse calculation method>> below.

1. [[funcmod-decomposition]]The total deformation is partitioned into one or more components.  The total deformation at any time and position is calculated by summing the contribution from each component. See <<discuss-components,discussion>> and <<formula-components, formulae>> below.

2. [[funcmod-component]]Each component comprises a spatial model and a time function. The spatial model defines displacement as a function of position.  The time function evaluates a scalar multiplier as a function of time.  The deformation contribution for the component at a given position and time is obtained by multipying the displacement components by the time function.

3. [[funcmod-spatial-extent]]The spatial model may be defined over a subset of the total extent of the model.  Where it is not defined it is assumed to be zero (see the <<discuss-no-data, discussion>> below).

4. [[funcmod-spatial-params]]The spatial model may define any or all of the following quantities: 
* horizontal displacement – the east and north displacement in metres or degrees
* vertical displacement – the upwards vertical displacement in metres.

+
Displacements not defined by the model are assumed to be zero.  Uncertainties not defined by the model take a default value defined in the spatial model or deformation model metadata.

4. [[funcmod-spatial-params-uncertainty]]The spatial model may define the uncertainty of the displacement in metres at each node.

+
Horizontal uncertainty can be represented by: 
* horizontal circular uncertainty e~h~
* east and north uncertainty e~e~, e~n~
* covariance matrix components [c~ee~ , c~en~, c~nn~].

+
Vertical uncertainty is represented by a single value e~v~.

+
Alternatively the uncertainty can be represented by a full covariance matrix at the node [c~ee~ , c~en~, c~nn~, c~eu~, c~nu~, c~uu~]

+
The parameters used to represent uncertainty will be the same for all spatial models in the deformation model and will be defined in the deformation model header.

+
The header will also define probability level for the uncertainty, examples would be "1 standard error", "95% confidence"

4. [[funcmod-spatial-params-quality]] Each node may have a quality parameter used to identify potential issues in the quality of the deformation model in the vicinity of the node.
For example it could indicate that there is surface faulting affecting cells adjacent to the node.  See the <<discuss-params-quality, discussion>> below.

4. [[funcmod-spatial-params-other]] A producer may include additional parameters at each node that will be ignored by compliant software.  The set of parameters must be the same for each node of a spatial model.

4. [[funcmod-nodata]] The displacements and uncertainties of a spatial model may evaluate to a _no data_ value at some locations, meaning that the deformation cannot be evaluated at that location.  At such locations the total deformation is undefined and cannot be calculated. See the <<discuss-no-data, discussion>> below.

10. [[funcmod-continuous-invertible]]
The displacement defined by the deformation model is required to be continuous and invertible within the spatial and temporal extent of the model except where it evaluates to _no data_.  
This is not enforced by the mathematical formulation of the deformation model.  It is a compliance requirement on producers of deformation models and can be assumed by implementors of software using the deformation model. See the <<discuss-continuous-invertible, discussion>> below.

7. [[funcmod-spatial-type]]The spatial model may be represented by either:
* a nested grid structure comprising one or more grids.  To evaluate the spatial model at a specific position the nesting algorithm identifies which grid is applicable at that position and the disclocation is interpolated from this grid alone.  The grids are constrained to be:
** Grids with rows equally spaced and columns equally spaced.
** Grid rows and columns are aligned with the axes of the definition CRS
** Grids are strictly nested.  Grids may share a common edge but otherise may not intersect unless one is a child grid fully contained by the other grid. The model may have more than one root grid (not contained in any other grid). 

+
See <<formula-bilinear-grid-interpolation, grid interpolation formulae>> below.
* a triangulated network comprising a set of control points at which the model quantities are defined and a list of triangles each defined by identifying the three nodes it uses.  The spatial model model is evaluated at a position by determining which triangle contains the position and interpolating on that triangle alone (where a point lies on an edge between trianges either may be used and will determine the same value). The triangle constraints are:
** triangles cannot overlap
** triangles cannot have zero area

+
See <<formula-linear-triangular-interpolation, triangle interpolation formulae>> below.

+
See the discussion of <<discuss-spatial-model, spatial model types>> below.

8. [[funcmod-time-type]]The time function is a scalar function of time calculated as the sum of one or more base functions.  Each base function may have a specified start and end epoch.  It evaluates to zero at times before the start epoch and at times equal to or after the end epoch.  Each base function can be:
 * a piecewise linear function of time defined by function values at ordered set of times.  The function is interpolated linearly between the specified values. Specialisations of the piecewise linear time function are:
 ** constant value
 ** constant velocity
 ** step function
 * a second order polynomial function of time (constant acceleration)
 * an exponential function
 * a logarithmic function
 * a sine or cosine function

+
Piecewise linear functions are defined by an ordered set of time/date values and a corresponding set of scale factors defining the value by which the spatial model is multiplied at that time. The functions are not necessarily continuous – for example the model may define step function.  The date/time values should be increasing.  Where there is a step function the series will include two consecutive identical date/time values.

+
See the <<discuss-time-function, discussion>> and <<formula-time-function, formulae>> for time functions below.

9. [[funcmod-14prm-transformation]]The deformation model specification may also include a 14 parameter
Bursa-Wolf transformation definition.  This is applied before the deformation components in a forward transformation or after them in an inverse transformation as described in the <<formula-14prm-transformation, formula>>.  
+
The 14 parameter transformation can be used to represent the bulk of the transformation between the source and target coordinate systems or epochs. This allows the more complex deformation components to efficiently represent perturbations from this movement which may be smaller and localised.  Note that jurisdictions may choose to implement this using an intermediate coordinate system rather including it than deformation model.  A deformation model should not be used to define a transformation which is fully described by a 14 parameter transformation.


11. [[funcmod-component-metadata]]Each component includes metadata defining:
* The type of spatial model (grid, triangulation)
* The spatial interpolation method to use
* The quantities it defines (displacements, uncertainties)
* A spatial definition of the extent of the spatial model (to determine if it is required at a specific position)
* (optional) default horizontal and vertical uncertainty which applies if the spatial model does not explicitly define uncertainty.
* (optional) text description of the source of the deformation represented in the model
* (optional) definition of areas where quality is impacted, for example where there is surface faulting.  The areas each include a description, multipolygon defining the extent of the affected area, and a start and end epoch for the event causing the unmodelled deformation. See the <<discuss-params-quality, discussion>> below.  

12. [[funcmod-model-metadata]]The deformation model includes metadata defining:
* The version of the deformation model specification with which the model complies
* The name of the model
* The version of the deformation model
* The publication date
* The licence under which the model is published
* Optional description of the model
* Contact information for the agency publishing the model
* Optional links to reference information about the model
* The source CRS definition (eg EPSG:xxxx)
* The target CRS definition (if the deformation model is implemented as a point motion model this will be the same as the source CRS).
* The spatial model definition CRS (the used to define the position at which the model is calculated)
* The reference epoch (note that each component defines a time function - this is not required to evaluate the model)
* The uncertainty reference epoch. Described <<discuss-uncertainty-epoch, below>>.
* The units of horizontal displacements
* The units of vertical displacements
* The format for representing uncertainty - which parameters are used at each node, and whet level of confidence.  For example horizontal covariance, vertical uncertainty, and 95% confidence level)
* The total spatial extent of the model 
* The time extent of the model
* The algorithm used to apply add the deformation to the reference position coordinates.

[[formulae]]
== Calculation formulae

This functional definition specifies the formulae that may be used to evaluate a model.  It is important that these formulae are well defined and consistently used so that publishers of deformation models can be confident they will be used correctly to obtain the expected deformation.

The formulae below are adopted from the JSON GeoTIFF specification.  This does not include formulae for interpolation on a triangle. Also it does not describe interpolating uncertainty other than represented by horizontal and vertical uncertainties e~h~, e~v~. In this specification the grid can be defined either in terms of a geographic (longitude/latitude) or projection (easting/northing) coordinate system.  Displacements and uncertainties are all in metres except that the horizontal displacement may be in degrees if the coordinate system is geographic.

[[formula-spatial-interpolation]]
=== Spatial interpolation

This proposal includes two suggested spatial model types - grid and triangulated models.  For each of these the value of a dimension (eg east displacement) at a specific evaluation point is evaluated by forming a weighted average of the values at a set of nodes P~1~, P~2~, .. with weights w~1~, w~2~.  For bilinear interpolation on a grid this is a set of four nodes on the corners of the grid cell within which the evaluation point lies.  Similarly for linear interpolation on a triangulated network this 
is a set of 3 nodes on the corners of the triangle within which 
the evaluation point lies.

The values at the nodes are combined using these weights as <<formula-weighted-node-combination, described below>>.  For the 
horizontal components (east and north displacement) two combination methods are defined - a simple weighted average and a geocentric combination which is applicable near the geographic poles where the east and north directions at the nodes may be markedly different.

[[formula-bilinear-grid-interpolation]]
==== Bilinear grid interpolation

Gridded spatial representations are defined as regular grids in terms of latitudes and longitudes.  That is, longitude (x) and latitude (y) of a grid node is defined as  

x~i~ = x~o~ + i.x~s~ +
y~j~ = y~o~ + j.y~s~ 


where  x~o~, y~o~ are the longitude and latitude of the southwest-most corner of the grid,  x~s~ and y~s~ are the longitude and latitude grid spacing, and i and j are the column and row number of the grid cell (where the west-most column and southernmost row are numbered 0).  Note that the longitude grid spacing need not be equal to the latitude grid spacing  – it is preferred that  x~s~ is approximately equal to y~s~/cos(y~m~), where y~m~ is the latitude of the middle of the grid, as this makes the grid cells approximately square (except at polar latitudes). 

Displacement vector elements are calculated using bilinear interpolation with respect to latitude and longitude from the nodes at the corners of the grid cell within which the calculation point lies.  Each element of the displacement is calculated independently (though of course the interpolation weighting will be the same for each, as they all refer to the same calculation point). 


Bilinear interpolation is defined as follows: 


The calculation point (x,y) is located in the grid cell between columns i and i+1, and rows j and j+1. 

[.right]
image::bilinear_interpolation.png[Alt=bilinear interpolation diagram,width=300,scalewidth=7cm]

The displacement elements (de, dn, du) at the calculation point are weighted means of the corresponding elements at the four nodes. 

The weights are calculated as follows: 


W~i,j~ = ((x~i+1~-x)/x~s~) * ((y~j+1~-y)/y~s~) +
W~i+1,j~ = ((x-x~i~)/x~s~) * ((y~j+1~-y)/y~s~) +
W~i,j+1~ = ((x~i+1~-x)/x~s~) * ((y-y~j~)/y~s~) +
W~i+1,j+1~ = ((x-x~i~)/x~s~)*((y-y~j~)/y~s~) +

[[formula-linear-triangular-interpolation]]
==== Linear triangular interpolation

The area covered by the spatial component is divided into triangles.  The spatial component is evaluated by first determining which triangle the evaluation point lies in.  The weights associated with each node of the triangle are defined by the following formulae:

TBC: _Need to add to formulae_

[[formula-weighted-node-combination]]
==== Calculation of weighted average of node values

Each interpolation method identifies a set of nodes P~i~ and weights W~i~.  At each node the spatial model may define any of the displacements de~i~, dn~i~, du~i~, and uncertainties eh~i~ and ev~i~.  For the horizontal displacement components de and dn two formulae are supported.  Generally a simple weighted average will suffice, but near the poles a more complex "geocentric weighted average" formulae describe below may be preferable.  This is also used for the vertical component du.

So for example the east displacement at the point (x,y) is calculated as  

// ... if we don't have latexmath then need alterative formulation representation

latexmath:[de = \sum_{i} W_i de_i]

// de = W~i,j~*de~i,j~ + W~i+1,j~*de~i+1,j~ + W~i,j+1~*de~i,j+1~ + W~i+1,j+1~*de~i+1,j+1~ 


The uncertainties eh, ev are interpolated using a weighted average of the variances eh~i~^2^, ev~i~^2^, for example 

latexmath:[eh = \sqrt{\sum_{i} W_i e_i^2}]

// eh = √(W~i,j~*eh~i,j~^2^ + W~i+1,j~*eh~i+1,j~^2^ + W~i,j+1~*eh~i,j+1~^2^ + W~i+1,j+1~*eh~i+1,j+1~^2^)

[[formula-geocentric-weighted-average]]
==== Geocentric weighted average

A simple average of the east and north displacements may not be appropriate where a grid cell spans a wide longitude range, typically for grids in polar regions.  In this situation the directions of the east and north vectors may be quite different at the set of nodes being averaged, as shown in the figure where the directions at nodes A and B and the evaluation point P are all quite different.  The impact of this is discussed below under <<discuss-geocentric-interpolation, errors of normal bilinear interpolation near poles>>.

[.right]
image::geocentric_bilinear_interpolation.png[Alt=geocentric bilinear interpolation diagram, width=200,scalewidth=7cm]

In such areas it may be more appropriate to base the grid on a local projected coordinate system. Also there is no issue even in polar regions if the grid has very narrow grid cells spanning a small longitude range. 

If grid cells spanning a large longitude range are used then a weighted average of the grid node displacements can be adapted by transforming the east and north vectors to a common direction before averaging them.

The geocentric interpolation method converts the displacement components from east and north components to geocentric X, Y, and Z components.  These are in the same direction and can be scaled and summed using the simple weighted avarage formulae above to calculate the X, Y, Z components of displacement at the calculation point, which are then converted back to components east and north at the calculation point.  

Note that this is only used to determine the horizontal displacement.  The vertical displacement and uncertainties are  computed using the simple weighted average formulae above. 


At longitude λ and latitude φ the dx, dy, dz values are calculated from the east and north displacements de, dn as: 


dx~i~ = -de~i~.sin(λ~i~) - dn~i~.cos(λ~i~).sin(φ~i~) +
dy~i~ = de~i~.cos(λ~i~) - dn~i~.sin(λ~i~).sin(φ~i~) +
dz~i~ = dn~i~.cos(φ~i~)


The X, Y, and Z directions are the same at any location, so the dx, dy, and dz displacements can be interpolated independently using bilinear interpolation as described above, eg:  


latexmath:[dx = \sum_{i} W_i dx_i]


The displacement at the calculation point is then calculated as: 


de = -dx.sin(λ) + dy * cos (λ) +
dn = -dx.cos(λ).sin(φ) - dy.sin(λ).sin(φ) + dz.cos(φ) 


[[formula-time-function]]
=== Time functions

The time function for a component defines a scale factor f(__t__) applied to component displacements at time __t__.  The deformation model metadata defines a temporal extent of the model from T~min~ to T~max~.  Time functions are considered undefined and the model cannot be evaluated for times before T~min~ and for times at or after T~max~.

The time function is evaluated as a sum of one or more base functions f~i~(t), each of which is defined for a range of times t~i,min~ &#8804; t < t~i,max~.  The base function evaluates to zero for times T~min~ &#8804; < t~i,min~ and for times t~i,max~ &#8804; < t < T~max~.

Following conventional use in deformation models the time functions are defined as a functions of decimal years (eg velocities are metres/year).  All date/time values such as calculation epoch, velocity reference epoch) are converted to decimal years for use in the following formulae.  The conversion to decimal years is done by first converting the epoch to UTC.   The year number _yyyy_ of the UTC epoch forms the integer part of the decimal year.  The fractional part of the decimal year is determined by dividing the number of seconds between  _yyyy_-01-01T00:00:00Z and the epoch by the number of seconds between _yyy1_-01-01T00:00:00Z and _yyyy_-01-01T00:00:00Z, where _yyy1_ is _yyyy_+1 (ie dividing the number of seconds since the start of the epoch UTC year by the total number of seconds in the epoch UTC year). 

For the uncertainties eh, ev the magnitude of the uncertainty is determined relative to the <<discuss-uncertainty-epoch, uncertainty reference epoch>>. The scale factor to apply to uncertainties is f~e~(__t__) is defined as  √abs(f(__t__)-f(t~0~)) where t~0~ is the uncertainty reference epoch of the model. 

The time function is defined as the sum of one or more of the following functions:

[cols="2,5a,5a"]
[options="header"]
|===
|Function type|Parameters|Formula (t~i,min~ &#8804; __t__ < t~i,max~)
|constant|None|f(__t__)=1
|velocity|Reference epoch t~0~|f(__t__) = (t - t~0~) all values of t
|acceleration|Reference epoch t~0~ +
Acceleration __a__|f(__t__) = (t - t~0~) + __a__.(t - t~0~)^2^ all values of t
|step | Reference epoch t~0~ |f(__t__) = 0 when t < t~0~,
|reverse step | Reference epoch t~0~ |f(__t__) = -1 when t < t~0~,
|piecewise|Time/factors corner points +
t~i~,f~i~ for i=0..n|
f(__t__) = 0 for __t__ < t~0~

f(__t__) = (f~i~.(t~i+1~ – __t__) + f~i+1~.(__t__-t~i~))/(t~i+1~-t~i~) +
for t~i~ < __t__ < t~i+1~

f(__t__) = 0 for __t__ > t~n~

|exponential|Start epoch t~0~ +
End epoch t~1~ +
Decay constant θ| f(__t__) = f~p~     when __t__ < t~0~ 

f(__t__) = f~0~ + (f~∞~-f~0~).(1 - exp(-(__t__-t~0~)/θ))     when t~0~ <= __t__ < t~1~ 


f(__t__) = f~0~ + (f~∞~-f~0~).(1 - exp(-(t~1~-t~0~)/θ))     when __t__ >= t~1~
|logarithmic|Reference epoch t~0~ +
Amplitude &#945; +
Decay rate &#964; | 0 for __t__ < t~0~ +
f(__t__) = &#945;.ln(1 + t/&#964;) for t~0~ &#8804; __t__ 
|cyclic|Frequency f (cycles per year) +
Reference epoch t~0~ +
Amplitude &#945; | f(__t__) = &#945;cos(f(__t__ - t~0~)/2&#120587;)
|===

Note that step, reverse step, and piecewise functions are redundant in that they can all be compiled from constant and velocity base functions.  They are provided to more clearly represent the intent of these functions.  

Future versions of the specification may add new base functions as required (for example to support slow slip events).


[[formula-components]]
=== Combination of components

To calculate the total deformation at a time and location, the displacement and uncertainties due to each component are calculated independently and then combined using the formulae below to obtain the total displacement and uncertainty at a location.  The total displacement is then applied to the coordinate. 


The same input position coordinate is used for each component - the components are not applied sequentially (ie the coordinate is not updated by the first component before being used to calculate the deformation on the second component). See the discussion below on <<discuss-parallel-calculation, using the same position for each component>>.


At a given time and location the elements from each component are combined to determine the overall displacement and errors. 


The displacement elements de, dn, dh are combined by simply adding their values calculated for each component.  For example, if there are n components for which the spatial representation calculates de as de~1~, de~2~, … to de~n~, and the time function evaluates to f~1~, f~2~, … to  f~n~ then the total model value for de is  


de = f~1~.de~1~+ f~2~.de~2~+ … + f~n~.de~n~ 


The error values eh, ev are combined by determining the root sum of squares (RSS) of the values determined for each component.  So for example 


eh = √(f~1~^2^.eh~1~^2^+ f~2~^2^.eh~2~^2^+ … + f~n~^2^.eh~n~^2^) 

=== Applying the offset to a coordinate

For each physical point on the surface for which the deformation model is defined the model defines the trajectory of that point in the target coordinate system.  The trajectory is defined for any valid epoch by adding the displacement calculated from the model to a reference coordinate for the point - a time invariant  coordinate for the point at which displacement is zero. The reference coordinate is not directly measurable - it is accessed by measuring a position in the target coordinate system and then subtracting the displacement from the deformation model at the epoch of measurement using the <<formula-inverse, inverse formula>> below.

The method used to add the calculated displacement to the reference coordinate is defined in the deformation model metadata.  Two methods are proposed -  _addition_ and _geocentric addition_.  The _addition_ method is relatively simple and adds the offset to the coordinates, converting metres to degrees first if necessary.  The _geocentric_ method is an alternative method that may be used near the poles if the grid latitude spacing is relatively large and accounts for the potentially significant differences in the north and east directions at each grid node.  It is only applicable if the offsets are defined in metres and the coordinate system is a geographic (latitude/longitude) system.   The _addition_ and _geocentric addition_ methods are detailed below.

==== Addition method

The method of the calculated east/north/up displacement to a coordinate depends on the units of the displacement and the type of the source and target coordinate system.  Also for geographic coordinate systems the method described here does not apply very close to the poles.  See the section below “calculation horizontal deformation near the poles” for details. 


If the source and target coordinate systems are projected coordinate systems then the units must be metres and the east and north displacements are simply added to the easting, northing ordinate. 


If the source and target coordinate systems are geographic coordinate systems and the east and north displacement units are degrees, then again the displacements are added to the longitude and latitude. 


If the source and target coordinate systems are geographic coordinate systems and the east and north displacement units are metres then the displacement components must be converted to degrees before they are added to longitude and latitude.  The conversion from metres to degrees requires the ellipsoid parameters of the geographic coordinate system. 


If a is the ellipsoid semi-major axis (eg 6378137.0), f is the flattening  (eg 1.0/298.25722210), λ is the latitude, and φ is the longitude then corrections to longitude and latitude (in radians) are given by: 


b = a.(1-f) +
dλ = de.√(b^2^sin^2^(φ)+a^2^cos^2^(φ))/a^2^cos(φ) +
dφ = dn.(b^2^sin^2^(φ)+a^2^cos^2^(φ))^3/2^/a^2^b^2^ 


The vertical displacement is always in metres and is simply added to the height coordinate. 

==== Geocentric addition method

The geocentric method can be applied  if the model is using a geographic coordinate system  and offsets measured in metres.  In this case the horizontal offset is converted to a geocentric (XYZ) offset, added to the geocentric coordinate, and then converted back to geographic coordinates.  The vertical coordinate is always calculated by simple addition of the vertical displacement to the reference coordinate height. 

This method may be applicable for coordinates near the pole, where simple addition of offsets to the longitude/latitude may not give the desired result.  This is shown in the figure below in which the grey vector shows the result of adding an east displacement to the longitude coordinate, and the black vector shows the result applying the same east displacement in the direction of the eastwards vector at the reference coordinate. Close to the pole the eastward vector is different to changing the longitude coordinate.. . 

[.right]
image::near_pole_east_offset.png[Alt=near pole east offset,width=200,scalewidth=7cm]

Moving away from the pole this issue rapidly becomes insignificant.  For a point at distance R from the pole with a displacement d, the difference is approximately d*(1-cos(d/R)), or approximately d^3^/2R^2^.  So for example a 1m eastward displacement 10km from the pole would have an error of 10^-8^m.  This is only an issue very close to the pole! 

TBC _Add formulae for conversion lat/lon to XYZ and vice versa_

Standard formulae are used for the conversion of geographic coordinates to and from geocentric coordinates.   The initial ellipsoidal height is set to zero before converting, and the resultant ellipsoidal height is discarded. 

The horizontal components of displacement are converted to X,Y,Z components using the same formulae as described for the “geocentric_bilinear” method. 

While this method can be used at any location it is not recommended other than close to the poles. It is computationally very expensive compared to simply adding the offsets to longitude and latitude.   


[[formula-inverse]]
=== Calculation of the inverse deformation model

Calculating the inverse of the deformation model requires an iterative solution as the coordinate in the defintion coordinate reference system is required to evaluate the deformation model, but it is not known until the deformation has been calculated and applied to the input coordinate in the target coordinate reference system.   


The iteration is done by

* using the input coordinate as an initial estimate for the output coordinate
* at each iteration:
** apply the deformation model to the current estimate of the output coordinate
** calculate the difference between the calculated coordinate and the input coordinate
** subtract this difference from the current estimate solution to obtain the estimated solution for the next iteration
** if this  difference is less than the precision required for the inverse operation then finish

The calculation of the difference and the subtraction of the difference from the current estimate is done by the “addition” or “geocentric” method, as defined in the deformation model metadata. (Formulae are defined above.) 

See the discussion below on the <<discuss-inverse-iteration, iterating the inverse calculation>>. 


=== Calculation of deformation between two epochs

Calculating the deformation between two times is straightforward for the displacement elements de, dn, and du as it is simply the difference between the values calculated at each time.   


This approach is not appropriate for the error components eh, ev.  Uncorrelated errors are combined as a root sum of squares, but the errors of displacements calculated for one component calculated at different times are clearly correlated.   


While there is no mathematically correct way to define the errors without a much more complex error model, the following approach is recommended if these errors are required. 


The time function error factor of the difference between t~0~ and t~1~ is calculated for each component separately as f~e,t1-t0~ = √abs(f(t~1~)-f(t~0~)).  


The eh and ev values from the spatial representation of each component are multiplied by these time function error factor values and then combined as the root sum of squares to give the total error of the deformation between the two epochs. 


[[formula-14prm-transformation]]
=== Calculation of the 14 parameter transformation

If the model includes a <<funcmod-14prm-transformation, 14 parameter transformation>> then this is applied to the coordinates after the 
deformation model is calculated and applied in a forward transformation.  In an inverse transformation it is applied (in reverse) before the deformation model components area been applied to the coordinate.

TBC _The 14 parameter transformation formulae need to be included here_ 

## Discussion points

[[discuss-components]]
### Decomposition into components

This specification assumes that the deformation can be decomposed into a set of spatial functions each multiplied by a scalar time function.  This is suitable for many geophysical phenomena such as secular motion (velocity models) and coseismic ground deformation. 

It may be less suitable to deformation with a complex time evolution such as slow slip events propogating along a fault system, or post-seismic deformation.  However currently deformation models for coordinate operations are all represented in this way.  Decomposing in this way can represent any deformation to an arbitrary level of detail, but it may not be the most efficient way to do so.

On a global scale plate motion models are used in some applications, in which the surface is split into a number of regions on each of which a rigid body rotation around the centre of the earth is defined.  Typically these are used in GNSS analysis to predict the movement of survey marks.  They are discontinuous at the boundaries between plates.  While these are a form of deformation model they are considered out of scope for this work.

Other approaches could be used to representing deformation in the future, such as triangular interpolation between points with individually defined trajectories. 

[[discuss-spatial-model]]
### Spatial model types

In practice nearly all current deformation models use grid representations.  There is a small usage of triangulated models which is included in this functional model specification.  This is a departure from the JSON-GeoTIFF format that this specification was originally written for.  

In New Zealand triangulated models were considered for modelling the deformation due to the 2011/12 Christchurch earthquakes but did not offer much advantage in the size of the model, and also are much less efficient to evaluate since it is necessary to search the triangulation to determine which triangle applies at a location.  (See https://www.linz.govt.nz/system/files_force/media/file-attachments/winefield-crook-beavan-application-localised-deformation-model-after-earthquake.pdf?download=1). 

However there are deformation models in use which are based on triangulated networks.

In the future there may be value in using some other representations for the spatial model.  For example structures such as Discrete Global Grid Systems provide a global grid of varying level of detail.  As these acquire more support in software and if there is a drive to develop a global deformation model then this may be worth developing. 


[[discuss-no-data]]
### No data and zero values

Most deformation models only cover a limited part of the globe.  They are limited by jurisdictions area of authority or by lack of data - for example areas of sea where there is no measure of deformation.  Also in areas affected by recent large deformation we may not have good data before the event.  There are several approaches to both identifying and handling these areas where there of no or poor information.  

The areas where displacement is not defined could be defined by a complex geographical extent.  However grids must be defined for rectangular regions in their coordinate system, which are unlikely to match the extent of the model.  From an implementor's point of view it is preferable that complex extents are identified by special grid values that identify points beyond the extent rather than a complex bounding shape.  

This specification supports the notion of a "no data" value. Note that an alternative approach is to specify an unreal value with a large uncertainty.  This is considered below in the  <<discuss-continuous-invertible, discussion on continuity of the model>>.  These "no-data" values could be identified in a number of ways, for example by a flag value on the grid nodes, by special values of displacement (eg 99999), or NaN (not a number) floating point values.  


The "no data" value is different from a zero displacement.  A value of zero is used where there is no significant displacement. A "no-data" value is used where the deformation is unknown, and might be significant.  

If evaluating the deformation model at a given location and time requires using a "no data" value then the displacment (or uncertainty) calculated at that location is undefined.  This would typically results in an error message to users to this effect.  The diagram below shows how this might look in a deformation model.  In this diagram the square marks the total extent of the deformation model.  Outside this area the deformation model cannot be evaluated.  In the deformation model is a gridded spatial model.  Within there are a number of nodes at which the displacement is not defined (that is it has a "no-data" value).  Where these nodes are required to calculate the displacement, which is any grid cell they are on the boundary of, the deformation model cannot be calculated.  The grey area in this diagram shows the region in which the displacement is not defined by the model and cannot be calculated.  

image::no_data.png[Alt=no data value diagram, width=400,scalewidth=9cm]

This may occur where the area in which deformation is defined is an irregular shape.  It might include coastal 
regions where the deformation of the seabed is not measured, or it may be that it crosses a jurisdictional boundary.  As the gridded model is by definition a rectangular area it will include these regions in which the deformation is not known, which are correctly represented by a "no data" value.

Where a component only covers a subset of the total area of a deformation model it is assumed to have zero displacement beyond its extent.  An example of this is a deformation model component representing deformation due to an earthquake.  In the vicinity of the epicentre there may be extensive deformation.  However there may also be large regions that lie within the extent of the deformation model but at which the deformation is zero or insignificant.  The deformation component created to represent this only needs to include the area where there is significant deformation.  This is shown in the figure below.  In this diagram the outer square represents the total extent of the deformation model.  Beyond this extent the deformation is undefined.  The nested grid inside the total extents is used to represent the deformation due to an earthquake.  In the region outside the nested grid component the deformation due to this component is zero. 

image::patch_extents.png[Alt=patch extent diagram, width=400,scalewidth=9cm]

[[discuss-params-quality]]
### Quality parameters at spatial model nodes

The project team identified an interest in having a quality parameter defined at nodes.  The main driver is to identify where there is surface faulting which where the deformation includes significant distortion or discontinuities that are not well represented by the deformation model.  

The intention is that software could warn users when the coordinate conversion quality is compromised by such distortion.

This could be represented by a quality parameter the corner nodes of affected grid cells.  Software could then assess the impact on an interpolated coordinate conversion by compiling the quality information from each of the nodes used in the interpolation.

There are some unresolved issues in using the quality parameter, including:
* how should it be represented 
* how should the measure be interpolated to provide a quality measure at an interpolated point
* how should the quality measure relate to the time (or times) of a conversion.  If time function evaluates to zero it should clearly be ignored, but how large can the time function be before the quality parameter is considered significant.
* how does the quality parameter relate to uncertainty

It may also be worth considering alternative methods of defining affected areas. For example in the component header could include one or more areas of concern, each with a spatial definition as a multipolygon and an event time.

[[discuss-continuous-invertible]]
### Requirement for model to be continuous and invertible

The deformation model is required to be continuous and invertible within the spatial and temporal extent of the model except where it is not defined (ie "no data" value).  This is a practical requirement on deformation models within the context of coordinate transformations.  

This means that the deformation model cannot exactly represent the true deformation.  For example where deformation is due to surface faulting the actual deformation may not be continuous across a fault line.  

The actual deformation also may not be invertible (at least as a function of horizontal position only) in an area of thrust faulting where points originally on opposite sides of the fault may be moved to the same horizontal position (though at different heights).

However the purpose of this deformation model specification is not to exactly represent deformation, but to represent it to the extent that is useful within the context of coordinate transformations.  

At least for the initial release of a functional model specification it is proposed to require a compliant model is continuous and invertible within the extent of the model.  This simplifies implementations and avoids the need to specify the behaviour where the model is not continuous and invertible.

The requirement for continuity does have implications for how models are defined.  For example it means that in nested grids child grids must be aligned with the parent grid as shown below to ensure continuity at the edge of the child grid. (Note that an alternative approach would be to define child grids as independent deformation components with the same time function which would model a perturbation from the simple parent grid - in this case there would be no requirement for the models to be aligned).


image::nested_grid_alignment.png[Alt=Nested grid alignemnt, width=600,scalewidth=12cm]

A question for implementers is how to transform data that extends beyond the deformation model.  If, as is likely, the deformation is not zero at the edge of the model then there is discontinuity across the boundary.  There are a number of possible approaches to handling this in the deformation model functional model.

* Require that valid models should have zero deformation at the boundary.  Producers may have to create an artificial buffer around their area of interest and calculate an unreal deformation field that reduces to zero at the outer edge of the buffer.  The model could also include uncertainties which are larger in the buffer to indicate that this data is not reliable.
* Specify (or recommend) algorithms for transforming data beyond the edge of the model that smooth out the discontinuity.  Model metadata could include parameters to support the implementation, for example a width of the smoothed region.  The algorithms could also specify how uncertainty is calculated to reflect this.
* Specify that transformation of data beyond the extents of the deformation model is not permitted, and will result in an exception (or equivalently a no data value).
* Not specify a behaviour - implementors can choose if and how to transform data outside the extents of the model.  Transformations beyond the extent of the model would be considered out of scope of this functional model specification.

From a producer's perspective the third of these - fail if data beyond the model is transformed - is most correct.  Also producers may not be concerned about transformations beyond their jurisdication, so that any of the last three options could be acceptable.  In any case it is beyond their control.  The first option - building a model with information that is known to be incorrect - is not desirable.  While this might be mitigated to an extent by increasing the uncertainty of the model in these regions, in practice most current software does not consume or report uncertainty information, so the user may be misled to thinking that the transformation is accurate.

From a user's point of view having a transformation fail beyond the extent of the model could be undesirable.  For example they may have features or observations that include points both inside and outside the extent of the model which are observed at different times and which they want to compare accurately within the extent of the model.  Trimming the features to the extent before doing this would be inconvenient.  However they need to be aware of potential inaccuracy in the comparison beyond the model extent.  This could be further complicated if the features span more than one deformation model.  Until we have a global model there may be no good solution for this.  

Also from a user's point of view it is desirable that different implementations give the same result - implementation specific behaviour is not desirable.  

Currently this specification takes the producer's perspective - a transformation beyond the extents of the model should fail. However this is open to debate!

[[discuss-time-function]]
### Time functions

The proposed set of base time functions includes those commonly used in geophysical models, for example reference station coordinates in the International Terrestrial Reference Frame.  However in practice there may be little benefit in complex time models, as it is unlikely that the same time function will apply at all points in the area affected by, for example, post-seismic deformation.  That is to say that the actual time evolution at each point within the spatial model may have different attributes and parameterisation.  The deformation model component is necessarily an simplification attempting to best fit the actual deformation over its spatial and temporal extent. 

In the near future it is likely that we may generate far more complex and accurate models using technology such as CORS and InSAR.  The deformation model representing this would most likely have multiple components, each with its own spatial model and time function, rather than a complex time function applying to a single spatial model.  For example each year there could be an updated gridded spatial model.  The deformation at any epoch could be interpolated or extrapolated from the nearest to models (or as in Japan modelled with a step function for each year). This is in effect a three dimensional grid with dimensions latitude, longitude, and time.  It can be easily encoded into this functional model by constructing time functions for each grid that define the interpolation between one grid and the next.

////

This can be encoded using this functional model by a series of gridded spatial models with time functions as illustrated below to interpolate between them.

[.center]
image::annual_grid_time_func.png[Alt=Example annual displacement grid time function,width=500, scalewidth=10cm]
////

[[discuss-geoentric-interpolation]]
### Geocentric interpolation near poles

The geocentric weighted average method proposed <<formula-geocentric-weighted-average, above>> is proposed for use in near polar regions where east and north topocentric vectors at adjacent grid nodes are in significantly different directions.


[.right]
image::geocentric_bilinear_interpolation.png[Alt=geocentric bilinear interpolation diagram, width=200,scalewidth=7cm]

To estimate the error that could be incurred using simple bilinear interpolation and not accounting for this difference we can consider a case where the deformation is 1m northwards at point A, and zero at point B in the diagram above.  Let the longitude grid spacing be λ~s~ radians.  If the calculation point P is λ radians past A, then the magnitude of the interpolated vector will be (λ~s~-λ)/λ~s~.  The error of orientation will be λ radians (the difference between north at A and north at the calculation point).  So the vector error will be sin(λ).(λ~s~-λ)/λ~s~.  Approximating sin(λ) as λ, this has a maximum absolute value in the range (0,λ~s~) of λ~s~/2.  So for example with a grid longitude spacing of 1° this could result in a 2cm error in the 1m of deformation vector. 

////

Using the geocentric interpolation method to calculate the horizontal component does cause some “leakage” of the horizontal deformation into the vertical component, that is: 

du = dx.cos(λ).cos(φ) + dy.sin(λ).cos(φ) + dz.sin(φ) 

For the interpolation of vertical displacement du this method proposes using the same formulae as the bilinear interpolation method - that is simple bilinear interpolation of the du component.  However this leakage does result in a small loss of magnitude in the horizontal component. The reduction is approximately scaling by the cosine of the angle between the vertical at the calculation point and the vertical at each grid node.  For a grid cell of 1 degree extent this would result in a scale error of 0.2mm for a 1m deformation vector.  (Note that this is a 1 degree extent measured on the globe - not a 1 degree extent of longitude which may be much smaller near the poles).  This can be ignored without significant loss of accuracy.

////

[[discuss-parallel-calculation]]
### Sequential or parallel evaluation of components

These formulae use the same input coordinate to calculate the deformation for each component. 

An alternative approach that could be used is to apply components sequentially.  That is the first component is calculated and applied to the coordinate, and then the modified coordinate is used to calculate the second component, and so on.  This may result in a different final coordinate to the proposed method, as the second and subsequent components are evaluated at a different location. 


Neither method is more correct from a theoretical point of view.  The main reason for specifying one approach is to ensure that there is an “authoritative” correct value, particularly where the deformation model is used in the definition of a datum (as in New Zealand for example). 


If the components are an ordered sequence of discrete events then the sequential approach might seem more intuitive.  However this is not necessarily the case.  For example consider a model in which the first component is a velocity function and the second is a step at 2003-01-01. If the deformation is calculated at 2004-01-01, the velocity function is applied as at 2004, and then that coordinate is used for the step function. If the deformation is calculated at 2014-01-01, then the velocity function is applied as at 2014, and that different coordinate is used to interpolate the step function model.  This means that the contribution from the step function could be different even though nothing else has changed other than the evaluation epoch. 


In practice the choice of independent or sequential evaluation of components is very unlikely to make a significant difference to the coordinates - at worst it is very similar to that described below for the inverse method in relation to iterating the inverse calculation or not.  The choice of independent evaluation has some small advantages in calculation in that:

* using the same input coordinates is slightly more efficient as the calculated displacement only needs to be applied to the coordinate once.  This could be a significant difference if the horizontal displacement is applied using the “geocentric” method as described below.  It is insignificant if the displacement is applied by simple addition.
* using the same input coordinates for all components provides an opportunity for parallelising calculation of components.
* using the same input coordinates for each component allows optimising transformations between two versions of the deformation model as common components can be ignored.

[[discuss-inverse-iteration]]
### Significance of iteration for the inverse deformation model evaluation

The error of not iterating the inverse transformation can be tested for the New Zealand NZGD2000 deformation model.  The least smooth area of deformation in New Zealand is that affected by the 2016 Kaikoura earthquake.  As this has been updated by “reverse patching” the inhomogeneity of the deformation field primarily affects pre-earthquake transformations.  Testing across the fault zone finds that the maximum error from not iterating an inverse transformation of epoch 2000.0 coordinates is about 0.015 metres.  However this is in an area where the deformation model is very inaccurate - it is smoothed across the fault zone and will have errors of many decimetres. For transforming epoch 2019.0 coordinates the maximum error is about 0.000014 metres.   In the North Island in an area largely unaffected by episodic events the maximum error is about 0.00015 metres. 

Based on this result it is recommended that the inverse transformation is iterated.  It is likely that this will double computation time (it would be unusual to require more than two iterations). 

Note that this is not about creating a more accurate transformation - the differences are much less than the uncertainty in the deformation model.  The reason for iterating is to satisfy a user expectation that applying a transformation followed by the inverse transformation will result in coordinates that are materially unchanged. 

[[discuss-uncertainty-epoch]]
### Uncertainty reference epoch

This is the epoch relative to which uncertainties are calculated and is referenced in the formulae below.  This may be different to the model reference epoch.  As hypothetical example, in New Zealand the deformation model includes a velocity with reference epoch 2000.0, so in principle error at epoch 2019 would be 19 times the uncertainty of the velocity (which is expressed in metres per year).  However in practice the New Zealand geodetic control network was adjusted in 2018, when the order 0 (highest accuracy) control stations were accurately located by CORS observations, and the rest of the network was adjusted to bring it into alignment with these stations.  The CORS stations NZGD2000 coordinates were calculated from the ITRF coordinates using the deformation model.  So in effect the deformation model and geodetic control were recalibrated at 2018.  So the error in 2019 due to the velocity component is only 1 times the uncertainty of the velocity. 

The use of the uncertainty reference epoch presents a difficulty from the point of view of maintaining the deformation model.  The appropriate reference epoch for the uncertainty could change far more frequently than any other attribute of the model.   For example in New Zealand the national geodetic is periodically recalculated using the most current ITRF coordinates of the reference stations.  This will change the uncertainty reference epoch for the deformation model, but otherwise leave it unchanged.  It is debatable whether this should constitute a new version of the deformation model, or of the datum it relates to.   Since most users will not ever calculate or use the uncertainties it makes no practical difference. 

Perhaps the most sensible approach for software that used the uncertainty information is that it should be able to override the uncertainty reference epoch. 

Another alternative is to remove the uncertainty epoch from the model definition, in which case it would be a requirement of software calculating uncertainty to provide a reference epoch.

=== Conversion of coordinates between versions of the deformation model

A common source of confusion is coordinate transformations between different versions of a datum. 


For example in New Zealand the deformation model was recently updated from version 20171201 to 20180701. Technically this is equivalent to a new version of the datum. 


Users with a GIS datasetin terms of the 20171201 version of the datum might want to update the dataset to version 20180701. The user expectation is that this will generate correct version 20180701 coordinates of the features in the database. 


The critical thing in this transformation is that the coordinate epoch for the transformation is before the event(s) implemented in the update.  This is somewhat counter-intuitive. 


Generally the update should not change the coordinates. The reason for the update is typically a deformation event such as an earthquake. The earthquake coseismic deformation is added to the deformation model as a step function that applies for transforming coordinates for epochs after the event. This means that the NZGD2000 coordinate system tracks the movement of features fixed to the ground and therefore the NZGD2000 coordinates of these features are not changed by the earthquake. In this case the deformation model is unchanged before the earthquake. Transforming at an epoch before the earthquake will leave the coordinates unchanged which is what is required.. 


Close to faulting the distortion due to the earthquake can be too intense to be included in the coordinates. In that case the deformation model will be smoothed across the fault zone. However the deformation is still measured and is used to update the coordinates. It is also added to the deformation model using a reverse step function that applies a negative deformation that applies when transforming  coordinates for epochs before the earthquake. In this case transforming coordinates at an epoch before the earthquake will result in subtracting the reverse patch from the coordinates.  This adds the deformation to the coordinates, which again is the correct update to coordinates to transform them to the new version of the datum.

== Additional information

__ The following additional information may be included __

* __ example calculations __
* __ glossary __

== Related work

This functional model is based on that developed by Land Information New Zealand in 2013 to encode and publish the NZGD2000 deformation model (https://www.linz.govt.nz/data/geodetic-system/datums-projections-and-heights/geodetic-datums/new-zealand-geodetic-datum-2000-nzgd2000/nzgd2000-deformation-model[https://www.linz.govt.nz/data/geodetic-system/datums-projections-and-heights/geodetic-datums/new-zealand-geodetic-datum-2000-nzgd2000/nzgd2000-deformation-model].  

This is also similar to a previous enhancement request PROJ project in 2018 to develop a deformation model format (https://github.com/OSGeo/PROJ/issues/1001[https://github.com/OSGeo/PROJ/issues/1001]).  After much very informed discussion in that github issue the enhancement ultimately stalled as there were no clear candidate formats for implementation.

This document is largely copied from the document proposing the implementation of deformation in the PROJ software using a JSON+GeoTIFF format in a https://docs.google.com/document/d/1wiyrAmzqh8MZlzHSp3wf594Ob_M1LeFtDA5swuzvLZY/edit[shared goodle document].

== Acknowledgements

I am very grateful to suggestions from numerous reviewers who contributed to development of the JSON+GeoTIFF proposal on which this document is based.  In particular Kristian Evers in relation to algorithms for deformation the current PROJ +deformation method, and Even Rouault for many recommendations on metadata and practicalities of encoding.  

