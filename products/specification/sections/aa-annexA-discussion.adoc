
[appendix,obligation="informative"]
== Explanations and expanded discussion points

__Note__: This section is likely not appropriate for the final abstract specification

[[discuss-elements]]
=== Decomposition into elements

This specification assumes that the deformation can be decomposed into a set of spatial functions each multiplied by a scalar time function.  This is suitable for many geophysical phenomena such as secular motion (velocity models) and coseismic ground deformation.

It may be less suitable to deformation with a complex time evolution such as slow slip events propagating along a fault system, or post-seismic deformation. However currently deformation models for coordinate operations are all represented in this way. Decomposing in this way can represent any deformation to an arbitrary level of detail, but it may not be the most efficient way to do so.

[[discuss-spatial-model]]
=== Spatial function types

In practice nearly all current deformation models use grid representations. Consideration has been given to the inclusion of models based on triangulated networks, though the project team has decided not to include this capability in the initial specification.

In New Zealand triangulated models were considered for the deformation due to the 2011/2012 Christchurch earthquakes but they did not significantly reduce the size of the model (<<Winefieldetal2010>>). They are also less efficient to evaluate since it is necessary to search the triangulation to determine which triangle applies at a location. 

The project team is not aware of any triangulated deformation models either in use or proposed for coordinate operations. One datum transformation uses a triangulated model (Finland EPSG:2393 (KKJ / Finland Uniform Coordinate System) projected CRS to EPSG:3067 (ETRS89 / TM35FIN(E,N))). This is similar to transformations implementing deformation, and indicates that triangulated deformation models may play a role in the future.

The project team briefly considered including global plate motion models in the specification. These are defined by a set of polygons defining the extent of each tectonic plate. A rigid body rotational velocity around the center of the earth is defined for each plate. Plate motion models exhibit discontinuities at the boundaries between plates. These models are often used in GNSS analysis to predict the movement of survey marks. Since this specification focuses on regional deformation, plate motion models were considered out of scope for this work.

Another potential future development is the use of Discrete Global Grid Systems to define a global grid with a heterogeneous level of detail. As these acquire more support in software then this may be worth considering.

=== No data and zero values

Most deformation models are regional and often defined by jurisdiction. In areas affected by recent large deformation pre-event data may be lacking.. There are several approaches to identifying and handling such areas.

Areas where displacement is undefined could be identified by a polygonal geographical extent. However, grids must be defined by rectangular regions, which are unlikely to match the extent of the known deformation. To avoid grids being used in areas where the deformation is undefined it is desirable that special “no-data” grid values can identify points these locations..

This specification supports the notion of a “no data” value. An alternative approach is to specify a value with a large uncertainty. This is considered below in the link:#4jpj0b3[_discussion on continuity of the model_]. “No-data” values could be identified in a number of ways, for example by a flag value on the grid nodes, by special values of displacement (e.g. 99999), or by NaN (not a number) floating point values.

The “no data” value differs from a zero displacement. A value of zero is used where there is no significant displacement. A “no-data” value is used where the displacement is unknown.

If any element of the model evaluates to "no-data" at a given location and time then the displacement (or uncertainty) at that location is undefined. <<image-no-data>> illustrates the use of “no-data” values in the model. In this figure the grey area shows the region where displacement is undefined and the square marks the total extent of the model. Outside this area the model cannot be evaluated. Within the gridded spatial function there are a number of nodes at which the displacement is not defined, these are identified with a “no-data” value. Where these nodes are required to interpolate a displacement, which is any grid cell they are on the boundary of, the displacement cannot be calculated..

[[image-no-data]]
image::no_data.png[title=Influence of "no data" values, width=400,scalewidth=9cm]

The grey area, where displacement is undefined, might include coastal regions where the deformation of the seabed is not measured, or it may cross a jurisdictional boundary. Since the grid is rectangular, it may include regions where the deformation is not known, which are represented in the model by a “no data” value.

Where an element only covers a portion of the total area of a deformation model the element is assumed to have zero displacement beyond its extent. This is common in deformation elements that include earthquake deformation. In the vicinity of the epicenter there may be extensive deformation. However, there may also be large regions within the extent of the deformation model where the deformation is zero or insignificant. The element representing this only needs to include the area where there is significant deformation. This is shown in <<image-patch-extent>>. In this figure the outer white box defines the total extent of the deformation model. Beyond this the deformation is undefined. The nested grid inside the model represents deformation due to an earthquake. In the region outside the nested grid the deformation from this element is zero.

[[image-patch-extent]]
image::patch_extents.png[title=A "patch" element covering a subset of the total model extent, width=400,scalewidth=9cm]

[[discuss-params-quality]]
=== Quality parameters at spatial function nodes


The project team identified an interest in having a quality parameter defined at nodes. The main driver is to identify where there is surface faulting causing distortion or discontinuities that are not well represented by the deformation model.

The quality parameter could be implemented in software to warn users when the coordinate conversion quality is compromised by such distortion.

Two different options for encoding a quality parameter have been considered. One is to define a quality measure parameter at each grid node. Another is to define this in the model metadata with a list of polygons defining areas of poor quality, each with an associated time at which the distortion occurred.

There are unresolved issues in using the quality parameter, including:

* if the parameter is defined for grid nodes how should it be interpolated to provide a quality measure at an interpolated point and how should the time of conversion be used? If the time function evaluates to zero it should clearly be ignored. However it is not clear how the parameter will apply when the time function is very close to zero.
* Uncertainty parameters give a quantitive measure of quality of the model. How should the quality parameter affect calculating the uncertainty of the deformation?

[[discuss-continuous-invertible]]
=== Requirement for the model to be continuous and invertible

The model should be continuous and invertible within its spatial and temporal extent except where it is not defined (i.e. “no data” value). This is a practical requirement of such models in the context of coordinate transformations.

This means that the model cannot represent the true deformation exactly. For example, where there is surface faulting the actual deformation may not be continuous across a fault line. The deformation also may not be invertible (at least as a function of horizontal position) in areas of thrust faulting where points originally on opposite sides of the fault may be moved to the same horizontal position (though at different heights).

The purpose of this deformation model specification, however, is not to represent deformation exactly, but to represent it to the extent useful within the context of coordinate transformations.

At least for the initial release of a model specification it is proposed to require that a compliant model is continuous and invertible within the extent of the model. This simplifies implementations and avoids the need to specify the behavior where the model is not continuous and invertible.

The continuity requirement has implications for how models are defined. For example, in nested grids, child grids must be aligned with the parent grid to ensure continuity at the edge of the child grid. This is illustrated in <<image-nested-grid>>.

[[image-nested-grid]]
image::nested_grid_alignment.png[title=Alignment of nested grids, width=600,scalewidth=12cm]

An alternative method of implementing more detail close to a fault would be to create another element with the same time function as the parent grid. This could have a much smaller extent and just model the perturbation of the displacement field from the parent grid.  It would evaluate to zero at its edge, and there would be no requirement for it to be aligned with the parent grid.

Software implementations of the model may need to transform data that extends beyond the model boundary. If, as is likely, the deformation is not zero at the edge of the model then there is discontinuity across the boundary. There are a number of possible approaches to handling this including:

* Require that valid models have zero deformation at the boundary. Deformation model producers may have to create an artificial buffer around their area of interest and calculate an unreal deformation field that reduces to zero at the outer edge of the buffer. The model could also include uncertainties which are larger in the buffer to indicate that data is not reliable there.
* Specify (or recommend) algorithms for transforming data beyond the edge of the model that smooth out the discontinuity. Model metadata could include parameters to support the implementation, for example a width of the smoothed region. The algorithms could also specify how uncertainty is calculated to reflect this.
* Specify that transformation of data beyond the extent of the model is not permitted, and will result in an exception (or equivalently a “no-data” value).
* Do not specify a behavior —implementations can choose if and how to transform data outside the extent of the model. Transformations beyond the extent of the model are considered out of scope of this specification.

From a model producer’s perspective the third of these options, fail if transformation beyond the model extent is attempted, is the correct approach. Also, model producers may not be concerned about transformations beyond their jurisdiction, so that any of the last three options could be acceptable. The first option — building a model with information that is known to be incorrect — is not desirable. While this might be mitigated to an extent by increasing the uncertainty of the model in these regions, most current software does not consume or report uncertainty information, so the user may be misled into thinking that the transformation is accurate.

From a user’s point of view, having a transformation fail beyond the extent of the model could be undesirable. For example, they may have features or observations that include points both inside and outside the extent of the model which are observed at different times and which they want to compare accurately within the extent of the model. Trimming the features to the extent before doing this would be inconvenient. However, users need to be aware of potential inaccuracy in the comparison beyond the model extent. This could be further complicated if the features span more than one deformation model. Global deformation models may mitigate this problem, but no suitable models exist at the time of writing.

Also for users it is important that different software implementations give the same result.

[[discuss-time-function]]
=== Time functions

The proposed set of base time functions includes those commonly used in geophysical models. However in practice complicated time functions using multiple base functions may not represent the deformation much better than simple functions, as it is unlikely that the same time function will apply at all points in an area affected by, for example, post-seismic deformation. The actual time evolution may have different attributes and parameterization at different points. Any element represents a best fit simplification of the actual deformation over its spatial and temporal extent.

There is some redundancy in the selected set of base functions.  In particular a velocity function including a start and end epoch is functionally identical to a ramp function. However these two options are provided to support quite different use cases.

Typically the velocity function will not be specified with both a start time and an end time. It represents secular deformation and the displacements from the spatial function quantify deformation accumulating in one year regardless of the start and end time.  A velocity function with just a start time would be appropriate where a deformation event causes a velocity change of indefinite duration.

The ramp function is used for specific time bounded events. It more meaningfully represents specific deformation events as the spatial function displacements reflect the total deformation rather than the rate of deformation. Also it supports the step function specialization which is not possible with a velocity function (as the velocity is infinite). Multiple ramp functions can be combined to approximate any time evolution.

In the near future it is likely that we may generate far more complex and accurate models using technology such as CORS and InSAR.  The deformation model representing this would most likely have multiple elements, each with its own spatial function and time function, rather than a complex time function applying to a single spatial function.  For example each year there could be an updated gridded spatial function.  The deformation at any epoch could be interpolated or extrapolated from the nearest to models (or as in Japan modelled with a step function for each year). This is in effect a three dimensional grid with dimensions latitude, longitude, and time.  It can be easily encoded into this model by constructing time functions for each grid that define the interpolation between one grid and the next.

////

This can be encoded using this model by a series of gridded spatial functions with time functions as illustrated below to interpolate between them.

[.center]
image::annual_grid_time_func.png[title=Example annual displacement grid time function,width=500, scalewidth=10cm]
////

[[discuss-geoentric-interpolation]]
=== Geocentric interpolation near poles

The geocentric weighted average method proposed in <<formula-geocentric-bilinear-interpolation>> is intended for use in near polar regions where east and north topocentric vectors at adjacent grid nodes differ significantly in orientation.


[[image-geocentric-bilinear-interpolation]]
image::geocentric_bilinear_interpolation.png[title=geocentric bilinear interpolation diagram, width=200,scalewidth=7cm]

To estimate the error that could be incurred using simple bilinear interpolation and not accounting for this directional difference, consider a case where the displacement is 1 meter northwards at point A in <<image-geocentric-bilinear-interpolation>>, and zero meters at point B. Let the longitude grid spacing be λ~s~ radians. If the calculation point P is λ radians past A, then the magnitude of the interpolated displacement will be (λ~s~-λ)/λ~s~. The error of orientation will be λ radians (the difference between north at A and north at the calculation point) and the displacement error will be sin(λ).(λ~s~-λ)/λ~s~. Approximating sin(λ) as λ, the error has a maximum absolute value in the range (0,λ~s~) of λ~s~/2. For example, with a grid longitude spacing of 1° the displacement error is about 2cm..

////

Using the geocentric interpolation method to calculate the horizontal component does cause some “leakage” of the horizontal deformation into the vertical component, that is:

du = dx.cos(λ).cos(φ) + dy.sin(λ).cos(φ) + dz.sin(φ)

For the interpolation of vertical displacement du this method proposes using the same formulae as the bilinear interpolation method - that is simple bilinear interpolation of the du component.  However this leakage does result in a small loss of magnitude in the horizontal component. The reduction is approximately scaling by the cosine of the angle between the vertical at the calculation point and the vertical at each grid node.  For a grid cell of 1 degree extent this would result in a scale error of 0.2mm for a 1m deformation vector.  (Note that this is a 1 degree extent measured on the globe - not a 1 degree extent of longitude which may be much smaller near the poles).  This can be ignored without significant loss of accuracy.

////

[[discuss-parallel-calculation]]
=== Sequential or parallel evaluation of elements

The calculation formulae above use the same input coordinate to calculate the deformation for each element.

An alternative approach that could be used is to compute deformation elements sequentially. In this approach, the first element is calculated and applied to the starting coordinate, and then the updated coordinate is used to calculate the second element, and so on. This may result in a different final coordinate to the proposed method, as the second and subsequent elements are evaluated at different locations.

Both approaches are correct from atheoretical point of view. The main reason for specifying one approach is to ensure that there is an “authoritative” correct value, particularly where the model is integral to the definition of a datum (as in New Zealand for example).

If the elements are an ordered sequence of discrete events then the sequential approach might seem more intuitive. However, this is not necessarily the case. For example, consider a model where the first element is a velocity function and the second is a step at 2003-01-01. If displacement is calculated at 2004-01-01, the velocity function is applied as at 2004, and then that coordinate is used for the step function. If the deformation is calculated at 2014-01-01, then the velocity function is applied as at 2014, and a different coordinate is used to interpolate the step function. This means that the contribution from the step function could be different even though nothing else has changed other than the evaluation epoch.

In practice the choice of independent or sequential evaluation of elements should yield an insignificant difference to the coordinates. Independent element evaluation has the following advantages:

* using the same input coordinates is slightly more efficient as the calculated displacement need only be applied to the coordinate once. This could make a significant difference if the horizontal displacement is applied using the “geocentric” method. Coordinate differences are insignificant if the displacement is obtained by simple addition.
* using the same input coordinates for each element provides an opportunity for parallelising calculation of elements.
* using the same input coordinates for each element allows optimising transformations between two versions of the deformation model since elements common to both versions can be ignored.


[[discuss-inverse-iteration]]
=== Significance of iteration for the inverse deformation model evaluation

The error incurred by not iterating the inverse transformation is evaluated for the New Zealand NZGD2000 deformation model.

The most complex deformation in New Zealand is in the Kaikoura region resulting from the 2016 Kaikoura earthquake. Coordinates here have been updated with “reverse patching” and the inhomogeneity of the deformation field primarily affects pre-earthquake transformations. Testing across the fault zone finds that the maximum error caused by not iterating an inverse transformation of epoch 2000.0 coordinates is about 0.015 meters. However, this error is in an area where the deformation model is inaccurate - it has been smoothed across the fault zone and exhibits errors of several decimeters. For transforming epoch 2019.0 coordinates the maximum error is well below 1 millimeter. In the North Island, an area largely unaffected by episodic events, the maximum error is about 0.2 millimeters.

Based on these results it is recommended to iterate the inverse transformation. Although this may increase computation time, in most cases two iterations will suffice While the iteration will not improve transformation accuracy, it ensures that the inverse transformation returns identical coordinates as its corresponding forward transformation.

Note that this is not about creating a more accurate transformation — the differences are much less than the uncertainty in the deformation model. The reason for iterating is to satisfy a user expectation that applying a transformation followed by the inverse transformation will result in coordinates that are materially unchanged.

=== Calculation of deformation between two epochs

The displacement de, dn and du to transform a coordinate between two epochs is simply the difference between the displacement values calculated at each epoch.

The uncertainties of these displacements require a more sophisticated calculation as uncertainty of displacement components calculated at different times from the same model are clearly correlated.

While there is no mathematically correct way to quantify the uncertainty without a more complete error model than defined in this deformation model representation, the following approach is suggested.

The time function error factor of the difference between t~0~ and t~1~ is calculated for each element separately as f~e,t1-t0~ = √abs(f(t~1~)-f(t~0~)).

The eh and ev values from the spatial function for each displacement element are multiplied by these time function error factors and then combined as the root sum of squares to give the total uncertainty of the displacement between the two epochs.

=== Conversion of coordinates between versions of the deformation model

A common source of confusion is coordinate transformations between different versions of a datum. For example, in New Zealand the deformation model was recently updated from version 20171201 to 20180701. Technically, this is equivalent to a new version of the NZGD2000 datum.

Transforming a dataset from one datum version is done using an epoch before the events that gave rise to the new version. This is illustrated in the two scenarios below.

Consider a GIS dataset referenced to the 20171201 version of the datum that requires updating to the 20180701 version. The reason for the update is typically a deformation event such as an earthquake. The earthquake coseismic deformation is added to the deformation model in the form of a step function used to transform coordinates at post-event epochs. Since the NZGD2000 coordinate system and deformation model tracks the movement of features fixed to the ground, the NZGD2000 coordinates of these features remain unchanged by the earthquake. The deformation model is also unchanged before the earthquake. Therefore, transforming features at any pre-event epoch will leave the coordinates unchanged.

Close to a fault plane, distortion due to the earthquake can be too intense to include in the coordinates. In this case, the deformation model will be smoothed across the fault zone. However, the deformation is still measured and used to update coordinates. It is added to the deformation model using a reverse step function that applies a negative displacement applicable for transforming coordinates at pre-event epochs. Here, pre-event epoch coordinate transformations invoke subtracting the reverse patch from the coordinates, which in turn adds the deformation to the coordinates. The final result is correct updated coordinates referenced to the new version of the datum.
