
[appendix,obligation="informative"]
== Specification rationale

__Note__: This section contains notes from the development of this specification that will most likely be removed from the final document.  These are retained here for the present until we need to remove them or find a better home for them!


[[discuss-spatial-model]]
=== Spatial function types

Audience: specification maintainers

Purpose: Present and explain choice

In practice nearly all current deformation models use grid representations. Consideration has been given to the inclusion of models based on triangulated networks, though the project team has decided not to include this capability in the initial specification.

In New Zealand triangulated models were considered for the deformation due to the 2011/2012 Christchurch earthquakes but they did not significantly reduce the size of the model (<<Winefieldetal2010>>). They are also less efficient to evaluate since it is necessary to search the triangulation to determine which triangle applies at a location. 

The project team is not aware of any triangulated deformation models either in use or proposed for deformation coordinate operations. One datum transformation uses a triangulated model (Finland EPSG:2393 (KKJ / Finland Uniform Coordinate System) projected CRS to EPSG:3067 (ETRS89 / TM35FIN(E,N))). This is similar to transformations implementing deformation, and indicates that triangulated deformation models may play a role in the future.

The project team briefly considered including global plate motion models in the specification. These are defined by a set of polygons defining the extent of each tectonic plate. A rigid body rotational velocity around the center of the earth is defined for each plate. Plate motion models exhibit discontinuities at the boundaries between plates. These models are often used in GNSS analysis to predict the movement of survey marks. Since this specification focuses on regional deformation, plate motion models were considered out of scope for this work.

Another potential future development is the use of Discrete Global Grid Systems to define a global grid with a heterogeneous level of detail. As these acquire more support in software then this may be worth considering.

[[discuss-params-quality]]
=== Quality parameters at spatial function nodes

Audience: specification maintainers

Purpose: Retain thoughts on undecided issue

Comment:  This is really a placeholder at the moment.  We haven’t reached a consensus on how this would be implemented. The primary use case is identifying areas near surface faulting where the actual deformation is not well represented by the model.  

See also:

* https://github.com/opengeospatial/CRS-Deformation-Models/issues/25
* https://github.com/opengeospatial/CRS-Deformation-Models/issues/29

The project team identified an interest in having a quality parameter defined at nodes. The main driver is to identify where there is surface faulting causing distortion or discontinuities that are not well represented by the deformation model.

The quality parameter could be implemented in software to warn users when the coordinate conversion quality is compromised by such distortion.

Two different options for encoding a quality parameter have been considered. One is to define a quality measure parameter at each grid node. Another is to define this in the model metadata with a list of polygons defining areas of poor quality, each with an associated time at which the distortion occurred.

There are unresolved issues in using the quality parameter, including:

* if the parameter is defined for grid nodes how should it be interpolated to provide a quality measure at an interpolated point and how should the time of conversion be used? If the time function evaluates to zero it should clearly be ignored. However it is not clear how the parameter will apply when the time function is very close to zero.
* Uncertainty parameters give a quantitative measure of quality of the model. How should the quality parameter affect calculating the uncertainty of the deformation?

[[discuss-time-function]]
=== Time functions

Audience: specification maintainers

Purpose: Present and explain choice

The proposed set of base time functions includes those commonly used in geophysical models. However in practice complicated time functions using multiple base functions may not represent the deformation much better than simple functions, as it is unlikely that the same time function will apply at all points in an area affected by, for example, post-seismic deformation. The actual time evolution may have different attributes and parameterization at different points. Any element represents a best fit simplification of the actual deformation over its spatial and temporal extent.

There is some redundancy in the selected set of base functions.  In particular a velocity function including a start and end epoch is functionally identical to a ramp function. However these two options are provided to support quite different use cases.

Typically the velocity function will not be specified with both a start time and an end time. It represents secular deformation and the displacements from the spatial function quantify deformation accumulating in one year regardless of the start and end time.  A velocity function with just a start time would be appropriate where a deformation event causes a velocity change of indefinite duration.

The ramp function is used for specific time bounded events. It more meaningfully represents specific deformation events as the spatial function displacements reflect the total deformation rather than the rate of deformation. Also it supports the step function specialization which is not possible with a velocity function (as the velocity is infinite). Multiple ramp functions can be combined to approximate any time evolution.

In the near future it is likely that we may generate far more complex and accurate models using technology such as CORS and InSAR.  The deformation model representing this would most likely have multiple elements, each with its own spatial function and time function, rather than a complex time function applying to a single spatial function.  For example, each year there could be an updated gridded spatial function.  The deformation at any epoch could be interpolated or extrapolated from the nearest to models (or as in Japan modeled with a step function for each year). This is in effect a three dimensional grid with dimensions latitude, longitude, and time.  It can be easily encoded into this model by constructing time functions for each grid that define the interpolation between one grid and the next.

////

This can be encoded using this model by a series of gridded spatial functions with time functions as illustrated below to interpolate between them.

[.center]
image::annual_grid_time_func.png[title=Example annual displacement grid time function,width=500, scalewidth=10cm]
////

[[discuss-parallel-calculation]]
=== Sequential or parallel evaluation of elements

Audience: specification maintainers

Purpose: Present and explain choice

The calculation formulae above use the same input coordinate to calculate the deformation for each element.

An alternative approach that could be used is to compute deformation elements sequentially. In this approach, the first element is calculated and applied to the starting coordinate, and then the updated coordinate is used to calculate the second element, and so on. This may result in a different final coordinate to the proposed method, as the second and subsequent elements are evaluated at different locations.

From a theoretical point of view neither method is necessarily more correct. The main reason for specifying one approach is to ensure that there is an “authoritative” correct value, particularly where the model is integral to the definition of a datum (as in New Zealand for example).

If the elements are an ordered sequence of discrete events then the sequential approach might seem more intuitive. However, this is not necessarily the case. For example, consider a model where the first element is a velocity function and the second is a step at 2003-01-01. If displacement is calculated at 2004-01-01, the velocity function is applied as at 2004, and then that coordinate is used for the step function. If the deformation is calculated at 2014-01-01, then the velocity function is applied as at 2014, and a different coordinate is used to interpolate the step function. This means that the contribution from the step function could be different even though nothing else has changed other than the evaluation epoch.

In practice the choice of independent or sequential evaluation of elements should yield an insignificant difference to the coordinates. Independent element evaluation has the following advantages:

* using the same input coordinates is slightly more efficient as the calculated displacement need only be applied to the coordinate once. This could make a significant difference if the horizontal displacement is applied using the “geocentric” method. Coordinate differences are insignificant if the displacement is obtained by simple addition.
* using the same input coordinates for each element provides an opportunity for parallelizing calculation of elements.
* using the same input coordinates for each element allows optimizing transformations between two versions of the deformation model since elements common to both versions can be ignored.


[[discuss-inverse-iteration]]
=== Significance of iteration for the inverse deformation model evaluation

Audience: specification maintainers

Purpose: Present and explain choice

The error incurred by not iterating the inverse transformation is evaluated for the New Zealand NZGD2000 deformation model.

The most complex deformation in New Zealand is in the Kaikoura region resulting from the 2016 Kaikoura earthquake. Coordinates here have been updated with “reverse patching” and the inhomogeneity of the deformation field primarily affects pre-earthquake transformations. Testing across the fault zone finds that the maximum error caused by not iterating an inverse transformation of epoch 2000.0 coordinates is about 0.015 meters. However, this error is in an area where the deformation model is inaccurate - it has been smoothed across the fault zone and exhibits errors of several decimeters. For transforming epoch 2019.0 coordinates, the maximum error is well below 1 millimeter. In the North Island, an area largely unaffected by episodic events, the maximum error is about 0.2 millimeters.

Based on these results it is recommended to iterate the inverse transformation. Although this may increase computation time, in most cases two iterations will suffice While the iteration will not improve transformation accuracy, it ensures that the inverse transformation returns identical coordinates as its corresponding forward transformation.

Note that this is not about creating a more accurate transformation - the differences are much less than the uncertainty in the deformation model. The reason for iterating is to satisfy a user expectation that applying a transformation followed by the inverse transformation will result in coordinates that are materially unchanged.

[[discuss-edge-of-model]]
=== Transforming data beyond the deformation model

Audience: implementors, specification maintainers

Purpose: Retain thoughts on undecided issue

Software implementations of the model may need to transform data that extends beyond the model boundary. If, as is likely, the deformation is not zero at the edge of the model then there is discontinuity across the boundary. There are a number of possible approaches to handling this including:

* Require that valid models have zero deformation at the boundary. Deformation model producers may have to create an artificial buffer around their area of interest and calculate an unreal deformation field that reduces to zero at the outer edge of the buffer. The model could also include uncertainties which are larger in the buffer to indicate that data is not reliable there.
* Specify (or recommend) algorithms for transforming data beyond the edge of the model that smooth out the discontinuity. Model metadata could include parameters to support the implementation, for example a width of the smoothed region. The algorithms could also specify how uncertainty is calculated to reflect this.
* Specify that transformation of data beyond the extent of the model is not permitted, and will result in an exception (or equivalently a “no-data” value).
* Do not specify a behavior - implementations can choose if and how to transform data outside the extent of the model. Transformations beyond the extent of the model are considered out of scope of this specification.

From a model producer's perspective, the third of these options - fail if transformation beyond the model extent is attempted - is the correct approach. Also, model producers may not be concerned about transformations beyond their jurisdiction, so that any of the last three options could be acceptable. The first option - building a model with information that is known to be incorrect - is not desirable. While this might be mitigated to an extent by increasing the uncertainty of the model in these regions, most current software does not consume or report uncertainty information, so the user may be misled into thinking that the transformation is accurate.

From a user's point of view, having a transformation fail beyond the extent of the model could be undesirable. For example, they may have features or observations that include points both inside and outside the extent of the model which are observed at different times and which they want to compare accurately within the extent of the model. Trimming the features to the extent before doing this would be inconvenient. However, users need to be aware of potential inaccuracy in the comparison beyond the model extent. This could be further complicated if the features span more than one deformation model. Global deformation models may mitigate this problem, but no suitable models exist at the time of writing.

Also for users it is important that different software implementations give the same result.
